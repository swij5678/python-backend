name: Performance & Load Testing

on:
  schedule:
    # Run performance tests daily at 3 AM UTC
    - cron: '0 3 * * *'
  push:
    branches: [ main, master ]
    paths:
      - 'src/**'
      - 'requirements*.txt'
      - 'pyproject.toml'
      - 'Dockerfile'
  pull_request:
    branches: [ main, master ]
    paths:
      - 'src/**'
      - 'requirements*.txt'
      - 'pyproject.toml'
      - 'Dockerfile'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'smoke'
        type: choice
        options:
          - smoke
          - load
          - stress
          - spike
          - volume
      duration:
        description: 'Test duration (e.g., 5m, 10s)'
        required: false
        default: '5m'
      users:
        description: 'Number of virtual users'
        required: false
        default: '10'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  prepare-environment:
    name: Prepare Test Environment
    runs-on: ubuntu-latest
    
    outputs:
      test-url: ${{ steps.setup.outputs.test-url }}
      container-id: ${{ steps.setup.outputs.container-id }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build test image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: python-service:perf-test
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Start test application
      id: setup
      run: |
        # Start the application container
        CONTAINER_ID=$(docker run -d \
          --name perf-test-app \
          -p 8000:8000 \
          -e ENVIRONMENT=test \
          python-service:perf-test)
        
        echo "container-id=$CONTAINER_ID" >> $GITHUB_OUTPUT
        echo "test-url=http://localhost:8000" >> $GITHUB_OUTPUT
        
        # Wait for application to be ready
        echo "Waiting for application to start..."
        for i in {1..30}; do
          if curl -f http://localhost:8000/health >/dev/null 2>&1; then
            echo "âœ… Application is ready"
            break
          fi
          echo "Waiting... ($i/30)"
          sleep 2
        done

  smoke-performance-test:
    name: Smoke Performance Test
    runs-on: ubuntu-latest
    needs: prepare-environment
    if: github.event.inputs.test_type == 'smoke' || github.event.inputs.test_type == '' || github.event_name != 'workflow_dispatch'
    
    steps:
    - name: Install k6
      run: |
        sudo gpg -k
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
        
    - name: Run smoke test
      run: |
        k6 run --vus 1 --duration 30s - <<EOF
        import http from 'k6/http';
        import { check, sleep } from 'k6';
        import { Rate } from 'k6/metrics';
        
        export let errorRate = new Rate('errors');
        
        export default function () {
          let response = http.get('${{ needs.prepare-environment.outputs.test-url }}/health');
          
          let result = check(response, {
            'status is 200': (r) => r.status === 200,
            'response time < 200ms': (r) => r.timings.duration < 200,
            'body contains status': (r) => r.body.includes('healthy'),
          });
          
          errorRate.add(!result);
          sleep(1);
        }
        EOF

  load-performance-test:
    name: Load Performance Test
    runs-on: ubuntu-latest
    needs: prepare-environment
    if: github.event.inputs.test_type == 'load' || github.event_name == 'schedule'
    
    steps:
    - name: Install k6
      run: |
        sudo gpg -k
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
        
    - name: Run load test
      run: |
        USERS=${{ github.event.inputs.users || '10' }}
        DURATION=${{ github.event.inputs.duration || '5m' }}
        
        k6 run --vus $USERS --duration $DURATION --out json=load-test-results.json - <<EOF
        import http from 'k6/http';
        import { check, sleep } from 'k6';
        import { Rate, Trend } from 'k6/metrics';
        
        export let errorRate = new Rate('errors');
        export let responseTime = new Trend('response_time');
        
        export let options = {
          thresholds: {
            errors: ['rate<0.1'],
            http_req_duration: ['p(95)<500'],
            http_req_duration: ['p(99)<1000'],
          },
        };
        
        export default function () {
          // Health check endpoint
          let healthResponse = http.get('${{ needs.prepare-environment.outputs.test-url }}/health');
          let healthCheck = check(healthResponse, {
            'health status is 200': (r) => r.status === 200,
            'health response time < 100ms': (r) => r.timings.duration < 100,
          });
          
          errorRate.add(!healthCheck);
          responseTime.add(healthResponse.timings.duration);
          
          // API endpoints (simulate typical usage)
          let apiResponse = http.get('${{ needs.prepare-environment.outputs.test-url }}/api/items');
          let apiCheck = check(apiResponse, {
            'api status is 200': (r) => r.status === 200,
            'api response time < 500ms': (r) => r.timings.duration < 500,
          });
          
          errorRate.add(!apiCheck);
          responseTime.add(apiResponse.timings.duration);
          
          sleep(1);
        }
        EOF
        
    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: load-test-results.json

  stress-performance-test:
    name: Stress Performance Test
    runs-on: ubuntu-latest
    needs: prepare-environment
    if: github.event.inputs.test_type == 'stress'
    
    steps:
    - name: Install k6
      run: |
        sudo gpg -k
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
        
    - name: Run stress test
      run: |
        k6 run --out json=stress-test-results.json - <<EOF
        import http from 'k6/http';
        import { check, sleep } from 'k6';
        import { Rate } from 'k6/metrics';
        
        export let errorRate = new Rate('errors');
        
        export let options = {
          stages: [
            { duration: '2m', target: 10 },   // Ramp up
            { duration: '5m', target: 50 },   // Stay at 50 users
            { duration: '2m', target: 100 },  // Ramp up to stress level
            { duration: '3m', target: 100 },  // Stay at stress level
            { duration: '2m', target: 0 },    // Ramp down
          ],
          thresholds: {
            errors: ['rate<0.2'],
            http_req_duration: ['p(95)<2000'],
          },
        };
        
        export default function () {
          let response = http.get('${{ needs.prepare-environment.outputs.test-url }}/health');
          
          let result = check(response, {
            'status is 200': (r) => r.status === 200,
            'response time < 2000ms': (r) => r.timings.duration < 2000,
          });
          
          errorRate.add(!result);
          sleep(Math.random() * 2 + 1);
        }
        EOF
        
    - name: Upload stress test results
      uses: actions/upload-artifact@v3
      with:
        name: stress-test-results
        path: stress-test-results.json

  spike-performance-test:
    name: Spike Performance Test
    runs-on: ubuntu-latest
    needs: prepare-environment
    if: github.event.inputs.test_type == 'spike'
    
    steps:
    - name: Install k6
      run: |
        sudo gpg -k
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
        
    - name: Run spike test
      run: |
        k6 run --out json=spike-test-results.json - <<EOF
        import http from 'k6/http';
        import { check, sleep } from 'k6';
        import { Rate } from 'k6/metrics';
        
        export let errorRate = new Rate('errors');
        
        export let options = {
          stages: [
            { duration: '1m', target: 10 },   // Normal load
            { duration: '30s', target: 200 }, // Spike!
            { duration: '1m', target: 10 },   // Back to normal
          ],
          thresholds: {
            errors: ['rate<0.3'],
            http_req_duration: ['p(95)<3000'],
          },
        };
        
        export default function () {
          let response = http.get('${{ needs.prepare-environment.outputs.test-url }}/health');
          
          let result = check(response, {
            'status is 200': (r) => r.status === 200,
            'response time < 3000ms': (r) => r.timings.duration < 3000,
          });
          
          errorRate.add(!result);
          sleep(Math.random() * 1);
        }
        EOF
        
    - name: Upload spike test results
      uses: actions/upload-artifact@v3
      with:
        name: spike-test-results
        path: spike-test-results.json

  database-performance-test:
    name: Database Performance Test
    runs-on: ubuntu-latest
    needs: prepare-environment
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'volume'
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: Install k6
      run: |
        sudo gpg -k
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
        
    - name: Run database performance test
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/testdb
      run: |
        k6 run --vus 20 --duration 3m --out json=db-perf-results.json - <<EOF
        import http from 'k6/http';
        import { check, sleep } from 'k6';
        import { Rate } from 'k6/metrics';
        
        export let errorRate = new Rate('errors');
        
        export default function () {
          // Test database-heavy endpoints
          let createResponse = http.post(
            '${{ needs.prepare-environment.outputs.test-url }}/api/items',
            JSON.stringify({
              name: 'Test Item ' + Math.random(),
              description: 'Performance test item'
            }),
            { headers: { 'Content-Type': 'application/json' } }
          );
          
          let createCheck = check(createResponse, {
            'create status is 201': (r) => r.status === 201,
            'create response time < 1000ms': (r) => r.timings.duration < 1000,
          });
          
          errorRate.add(!createCheck);
          
          // Test list endpoint
          let listResponse = http.get('${{ needs.prepare-environment.outputs.test-url }}/api/items');
          let listCheck = check(listResponse, {
            'list status is 200': (r) => r.status === 200,
            'list response time < 500ms': (r) => r.timings.duration < 500,
          });
          
          errorRate.add(!listCheck);
          
          sleep(0.5);
        }
        EOF
        
    - name: Upload database performance results
      uses: actions/upload-artifact@v3
      with:
        name: db-performance-results
        path: db-perf-results.json

  memory-profiling:
    name: Memory & CPU Profiling
    runs-on: ubuntu-latest
    needs: prepare-environment
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'volume'
    
    steps:
    - name: Install profiling tools
      run: |
        sudo apt-get update
        sudo apt-get install -y sysstat htop
        
    - name: Monitor system resources
      run: |
        # Start monitoring in background
        iostat -x 1 300 > iostat.log &
        IOSTAT_PID=$!
        
        vmstat 1 300 > vmstat.log &
        VMSTAT_PID=$!
        
        # Monitor Docker container resources
        docker stats ${{ needs.prepare-environment.outputs.container-id }} --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}\t{{.BlockIO}}" > docker-stats.log &
        DOCKER_PID=$!
        
        # Run load test while monitoring
        sleep 10
        curl -X POST ${{ needs.prepare-environment.outputs.test-url }}/api/load-test &
        
        # Wait for monitoring to complete
        sleep 300
        
        # Kill monitoring processes
        kill $IOSTAT_PID $VMSTAT_PID $DOCKER_PID || true
        
    - name: Upload profiling results
      uses: actions/upload-artifact@v3
      with:
        name: system-profiling
        path: |
          iostat.log
          vmstat.log
          docker-stats.log

  cleanup-environment:
    name: Cleanup Test Environment
    runs-on: ubuntu-latest
    needs: [prepare-environment, smoke-performance-test, load-performance-test, stress-performance-test, spike-performance-test, database-performance-test, memory-profiling]
    if: always()
    
    steps:
    - name: Stop and remove test containers
      run: |
        if [ -n "${{ needs.prepare-environment.outputs.container-id }}" ]; then
          docker stop ${{ needs.prepare-environment.outputs.container-id }} || true
          docker rm ${{ needs.prepare-environment.outputs.container-id }} || true
        fi
        
        # Clean up any other test containers
        docker ps -q --filter "name=perf-test*" | xargs -r docker stop
        docker ps -aq --filter "name=perf-test*" | xargs -r docker rm

  analyze-results:
    name: Analyze Performance Results
    runs-on: ubuntu-latest
    needs: [load-performance-test, stress-performance-test, spike-performance-test, database-performance-test]
    if: always() && (needs.load-performance-test.result == 'success' || needs.stress-performance-test.result == 'success')
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
      
    - name: Install analysis tools
      run: |
        pip install pandas matplotlib seaborn
        
    - name: Analyze performance data
      run: |
        python3 << 'EOF'
        import json
        import pandas as pd
        import matplotlib.pyplot as plt
        import os
        
        def analyze_k6_results(filename):
            if not os.path.exists(filename):
                return None
                
            with open(filename, 'r') as f:
                data = [json.loads(line) for line in f if line.strip()]
            
            # Filter for http_req_duration metrics
            duration_data = [d for d in data if d.get('metric') == 'http_req_duration']
            
            if duration_data:
                durations = [d['data']['value'] for d in duration_data]
                return {
                    'avg_duration': sum(durations) / len(durations),
                    'min_duration': min(durations),
                    'max_duration': max(durations),
                    'p95_duration': sorted(durations)[int(len(durations) * 0.95)],
                    'total_requests': len(durations)
                }
            return None
        
        results = {}
        test_files = ['load-test-results.json', 'stress-test-results.json', 'spike-test-results.json', 'db-perf-results.json']
        
        for test_file in test_files:
            if os.path.exists(test_file):
                result = analyze_k6_results(test_file)
                if result:
                    test_name = test_file.replace('-results.json', '').replace('-', ' ').title()
                    results[test_name] = result
        
        print("ðŸ” Performance Test Results Summary:")
        print("=" * 50)
        
        for test_name, data in results.items():
            print(f"\n{test_name}:")
            print(f"  Total Requests: {data['total_requests']}")
            print(f"  Average Duration: {data['avg_duration']:.2f}ms")
            print(f"  Min Duration: {data['min_duration']:.2f}ms")
            print(f"  Max Duration: {data['max_duration']:.2f}ms")
            print(f"  95th Percentile: {data['p95_duration']:.2f}ms")
            
            # Performance assessment
            if data['p95_duration'] < 500:
                print("  âœ… Performance: GOOD")
            elif data['p95_duration'] < 1000:
                print("  âš ï¸  Performance: ACCEPTABLE")
            else:
                print("  âŒ Performance: NEEDS ATTENTION")
        
        # Save summary
        with open('performance-summary.json', 'w') as f:
            json.dump(results, f, indent=2)
        EOF
        
    - name: Upload analysis results
      uses: actions/upload-artifact@v3
      with:
        name: performance-analysis
        path: performance-summary.json
        
    - name: Create performance report issue
      if: github.event_name == 'schedule'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let summary = "Performance test completed but no detailed results available.";
          try {
            const data = fs.readFileSync('performance-summary.json', 'utf8');
            const results = JSON.parse(data);
            
            summary = "## Performance Test Results\n\n";
            for (const [testName, data] of Object.entries(results)) {
              const status = data.p95_duration < 500 ? 'âœ… GOOD' : 
                           data.p95_duration < 1000 ? 'âš ï¸ ACCEPTABLE' : 'âŒ NEEDS ATTENTION';
              
              summary += `### ${testName}\n`;
              summary += `- **Status**: ${status}\n`;
              summary += `- **Total Requests**: ${data.total_requests}\n`;
              summary += `- **Average Duration**: ${data.avg_duration.toFixed(2)}ms\n`;
              summary += `- **95th Percentile**: ${data.p95_duration.toFixed(2)}ms\n\n`;
            }
          } catch (error) {
            console.log('Could not read performance summary:', error.message);
          }
          
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Performance Test Report - ${new Date().toISOString().split('T')[0]}`,
            body: `${summary}
            
            **Test Run**: [${context.runId}](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
            **Date**: ${new Date().toISOString()}
            
            Review the detailed results in the workflow artifacts.`,
            labels: ['performance', 'automated', 'monitoring']
          });
